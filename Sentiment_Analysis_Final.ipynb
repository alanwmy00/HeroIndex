{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1102e64f-08cc-4934-9a4b-34a3dbacce5e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5ab10-3632-429d-bc01-8e71e9f5b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff5f89-cc9b-4bd8-9802-5ac335d6ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"labeled_data_final.txt\", encoding = 'unicode_escape')\n",
    "\n",
    "recode = {\n",
    "    \"positive\" : 0, \n",
    "    \"negative\" : 1\n",
    "}\n",
    "\n",
    "articles.sentiment = articles.sentiment.map(recode)\n",
    "labeled_subset = articles.dropna(subset = ['sentiment'])\n",
    "\n",
    "company = pd.read_csv(\"Company-2022-05-08.txt\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add(\"'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959a36c-c604-4a92-ae28-2b77628268ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    \"\"\"\n",
    "    Cleans the paragraphs within a dataframe of articles before processing text.\n",
    "    Requires the apply function on the column of text. \n",
    "    \"\"\"\n",
    "    # delete stopwords\n",
    "    temp = \" \".join(filter(lambda x: x not in stops, sentence.split()))\n",
    "    # remove non-english characters\n",
    "    temp = temp.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Delete excessive spaces and return\n",
    "    return re.sub(\"  \", \" \", temp)\n",
    "\n",
    "\n",
    "labeled_subset[\"clean_text\"] = labeled_subset.text.apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d72da-2bd2-4b97-aecb-e257fe06c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_companies(articles_data):\n",
    "    \"\"\"\n",
    "    Creates a row for individual companies within the articles dataset.\n",
    "    New rows are created for every value within the companies column that contains\n",
    "    a comma (e.g. \"31,287\").\n",
    "    Expected input is a dataframe of the articles with columns titled\n",
    "    \"companies\". \n",
    "    \"\"\"\n",
    "    # reset the index of articles data in case rows were manipulated\n",
    "    articles_data = articles_data.reset_index(drop = True)\n",
    "    \n",
    "    for row in range(len(articles_data)):\n",
    "        # pull the company ids\n",
    "        company_id = str(articles_data.companies[row])\n",
    "        \n",
    "        # fix if there are multiple companies in the same cell\n",
    "        if (\",\" in company_id):\n",
    "            multiple_companies = company_id.split(\",\") # create a list for each term split by commas\n",
    "            # create a dataframe that copies the original row for as many companies as are listed\n",
    "            sub_companies = pd.concat([articles_data.iloc[[row]]]*len(multiple_companies), ignore_index = True)\n",
    "            \n",
    "            # change each row to have its own company code\n",
    "            for j in range(len(sub_companies)):\n",
    "                sub_companies.at[j, 'companies'] = multiple_companies[j]\n",
    "            \n",
    "            # add the new dataframe to the old one\n",
    "            articles_data = pd.concat([articles_data, sub_companies], ignore_index = True)\n",
    "            \n",
    "            # drop the original row with multiple companies in the same cell\n",
    "            articles_data.drop(row, inplace = True)\n",
    "    \n",
    "    \n",
    "\n",
    "    return articles_data\n",
    "\n",
    "companies_seperated = seperate_companies(labeled_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1364f35-f4c7-49e0-a0fc-e908c08978f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_to_company(articles_data, company_data):\n",
    "  \n",
    "    \"\"\"\n",
    "    Expected inputs and expected columns:\n",
    "        articles_data, a dataframe of articles:\n",
    "            - \"id\"\n",
    "            - \"companies\"\n",
    "            - \"clean_text\"\n",
    "            \n",
    "        company_data, a dataframe of companies and their codes:\n",
    "            - \"id\"\n",
    "            - \"short_name\"\n",
    "    \n",
    "    Outputs a dataframe with the same columns as the articles_data input, but with an additional \"relevant_sentences column.\"\n",
    "    This column includes only sentences that directly reference the company for the use of targeted sentiment analysis. \n",
    "    \n",
    "    \"\"\"\n",
    "    final_sentences = []\n",
    "    # reset the index of articles data in case rows were manipulated\n",
    "    articles_data = articles_data.reset_index(drop = True)\n",
    "    \n",
    "    for row in range(len(articles_data)):\n",
    "        # pull the company short name from the company_data using articles_data\n",
    "        company_id = articles_data.companies[row]\n",
    "        index = company_data[company_data['id'] == int(company_id)].index[0]\n",
    "        company_name = company_data.short_name[index]\n",
    "        \n",
    "        # isolating the text to sentences where companies appear\n",
    "        look_for_string = r\"([^.]*?\" + company_name + \"[^.]*\\.)\"\n",
    "        relevant_sentences = re.findall(look_for_string,articles_data.clean_text[row])\n",
    "        final_sentences.append(\"\".join(relevant_sentences))\n",
    "        \n",
    "    articles_data[\"relevant_sentences\"] = final_sentences\n",
    "        \n",
    "    return articles_data\n",
    "\n",
    "df = isolate_to_company(companies_seperated, company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79f166-0f39-4e29-a8cb-9de94cb7f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_sentences_to_list(relevant_sentences):\n",
    "    \"\"\"\n",
    "    Helper function for apply method used in split_relevant_sentences().\n",
    "    Convert large string of sentences into list of strings for each sentences.\n",
    "    \"\"\"\n",
    "    l = [sentence for sentence in relevant_sentences.split(\".\")]\n",
    "    return l[:-1] #drop last element since it is always empty string ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcc5cf-7667-435f-88b0-b7648cd6d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_relevant_sentences(articles_data, company_data):\n",
    "    \"\"\"\n",
    "    Expected inputs and expected columns:\n",
    "        articles_data, a dataframe of articles:\n",
    "            - \"relevant_sentences\"\n",
    "            \n",
    "        company_data, a dataframe of companies and their codes:\n",
    "            - \"id\"\n",
    "            - \"short_name\"\n",
    "\n",
    "    Outputs a dataframe with the same columns as the articles_data input, \n",
    "    but with an additional \"relevant_sentences_list\" and \"split_sentences\" columns.\n",
    "    - \"relevant_sentences_list\" is a list of strings, where each element is a sentence from column \"relevant sentences\".\n",
    "    - \"split_sentences\" is list of dictionaries. All keys are left, center, right, where 'left' and 'right' are \n",
    "    portions of sentences to the left and right of the targeted company, and 'center' is the company name\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"relevant_sentences_list\"] = df.relevant_sentences.apply(relevant_sentences_to_list)\n",
    "    \n",
    "    final_dictionaries = []\n",
    "    # reset the index of articles data in case rows were manipulated\n",
    "    articles_data = articles_data.reset_index(drop = True)\n",
    "\n",
    "    for row in range(len(articles_data)):\n",
    "        # pull the company short name from the company_data using articles_data\n",
    "        company_id = articles_data.companies[row]\n",
    "        index = company_data[company_data['id'] == int(company_id)].index[0]\n",
    "        company_name = company_data.short_name[index]\n",
    "        \n",
    "        dict_list = [] #list for each article/company pair\n",
    "        for sentence in articles_data.iloc[row].relevant_sentences_list:\n",
    "            d = {'left'   : '',\n",
    "                 'center' : company_name,\n",
    "                 'right'  : ''}\n",
    "            split_sentence = sentence.split(company_name, maxsplit = 1) #take first occurance of company, if mentioned multiple times in single sentence\n",
    "            d['left']   = split_sentence[0]\n",
    "            d['right']  = split_sentence[1]\n",
    "            dict_list.append(d)\n",
    "        \n",
    "        final_dictionaries.append(dict_list)\n",
    "     \n",
    "    \n",
    "    articles_data[\"split_sentences\"] = final_dictionaries\n",
    "        \n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694cbc57-8153-4f1b-80a1-429c4aa3d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install NewsSentiment\n",
    "from NewsSentiment import TargetSentimentClassifier\n",
    "\n",
    "def get_sentiments(articles_data):\n",
    "    \"\"\"\n",
    "    Expected inputs and expected columns:\n",
    "        articles_data, a dataframe of articles:\n",
    "            - \"split_sentences\"\n",
    "\n",
    "    Outputs a dataframe with the same columns as the articles_data input, but with an additional \n",
    "    \"all_sentiments\" and \"targeted_sentiment\" columns.\n",
    "    - \"all_sentiments\" is a dictionary, with keys positive, neutral, negative. Values are averaged sentiment \n",
    "    scores from all relevant sentences in the article. Uses model from TargetSentimentClassifer from NewsSentiment library.\n",
    "    - \"targeted_sentiment\" is string either 'positive', 'neutral', 'negative' \n",
    "    determined by max value from column \"all sentiments\".\n",
    "    \"\"\"\n",
    "    tsc = TargetSentimentClassifier()\n",
    "    \n",
    "    final_dictionaries = []\n",
    "    highest_sentiments = []\n",
    "    for row in range(len(articles_data)):\n",
    "        n_sentences = len(articles_data.split_sentences[row])\n",
    "        \n",
    "        if n_sentences != 0:\n",
    "            d = {'positive' : 0,\n",
    "                 'neutral'  : 0,\n",
    "                 'negative' : 0}\n",
    "\n",
    "            for i in range(n_sentences):\n",
    "                #sentiment is list of dictionaries, where sentiment[0] has the highest probability, whether that be pos, neu, neg\n",
    "                sentiment = tsc.infer_from_text(articles_data.split_sentences[row][i]['left'],\n",
    "                                                articles_data.split_sentences[row][i]['center'], \n",
    "                                                articles_data.split_sentences[row][i]['right'])\n",
    "                ordered_sentiment = sorted(sentiment, key=lambda d: d['class_label'], reverse = True) #order so always in pos, neu, neg order\n",
    "                d['positive'] += ordered_sentiment[0]['class_prob']\n",
    "                d['neutral']  += ordered_sentiment[1]['class_prob']\n",
    "                d['negative'] += ordered_sentiment[2]['class_prob']\n",
    "\n",
    "            # average scores\n",
    "            d['positive'] = d['positive'] / n_sentences\n",
    "            d['neutral']  = d['neutral']  / n_sentences \n",
    "            d['negative'] = d['negative'] / n_sentences \n",
    "            final_dictionaries.append(d)\n",
    "            \n",
    "            #store highest average sentiment\n",
    "            highest_sentiments.append(max(d, key = d.get))\n",
    "        else:\n",
    "            final_dictionaries.append(dict(positive = None, neutral = None, negative = None))\n",
    "            highest_sentiments.append(None)\n",
    "            \n",
    "        \n",
    "    articles_data['all_sentiments'] = final_dictionaries\n",
    "    articles_data['targeted_sentiment'] = highest_sentiments\n",
    "    \n",
    "    return articles_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
